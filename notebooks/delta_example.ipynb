{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiere notwendige Bibliotheken\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle eine SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeExample\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle ein Beispiel DataFrame\n",
    "data = [('Alice', 25), ('Bob', 30), ('Charlie', 35)]\n",
    "columns = ['Name', 'Age']\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Schreibe das DataFrame als Delta-Tabelle\n",
    "df.write.format(\"delta\").save(\"/delta/example\")\n",
    "\n",
    "# Lese die Delta-Tabelle\n",
    "delta_df = spark.read.format(\"delta\").load(\"/delta/example\")\n",
    "\n",
    "# Zeige die Daten\n",
    "delta_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
